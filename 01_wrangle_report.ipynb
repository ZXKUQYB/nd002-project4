{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Wrangle and Analyze the Dataset of WeRateDogs Twitter Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will inspect the Twitter Archive of [@dog_rates](https://twitter.com/dog_rates) (aka [WeRateDogs](https://en.wikipedia.org/wiki/WeRateDogs)), and will show how to apply concepts and practices in course materials to tackle real-world data wrangling problems. This report is a part of Udacity's Data Analyst Nanodegree Program.\n",
    "\n",
    "As opposed to other projects, we will put emphasis on wrangling raw data, which is only a portion of the whole data analysis process. There will be 3 files needed in the wrangling task:\n",
    "\n",
    "* [Enhanced WeRateDogs Twitter Archive](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv) ([Backup URL 1](https://video.udacity-data.com/topher/2018/November/5bf60c1e_twitter-archive-enhanced-2/twitter-archive-enhanced-2.csv))\n",
    "* [Tweet Image Predictions File](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv) ([Backup URL 1](https://video.udacity-data.com/topher/2018/November/5bf60c69_image-predictions-3/image-predictions-3.tsv))\n",
    "* [Additional Data via the Twitter API](https://video.udacity-data.com/topher/2018/November/5be5fb7d_tweet-json/tweet-json.txt) ([Backup URL 1](https://video.udacity-data.com/topher/2018/November/5bf60cc8_tweet-json/tweet-json)) ([Backup URL 2](https://video.udacity-data.com/topher/2018/November/5bf60d25_tweet-json/tweet-json.zip))\n",
    "\n",
    "Ideally the 3rd file should be downloaded directly from Twitter using their API interfaces, but in case we could not gain access to a Twitter developer account (which is very likely due to their tightened verification measures), there is one more file we need to download (prepared by Udacity's staff members).\n",
    "\n",
    "* [The Twitter API code to gather data](https://video.udacity-data.com/topher/2018/November/5be5fb4c_twitter-api/twitter-api.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess and clean the dataset properly, we need to know attribute (or \"column\" when we load data into Pandas DataFrames) definition of downloaded Tweet data. Unfortunately there is only [scarce information](https://chrissardegna.com/blog/using-your-twitter-archive-to-visualize-your-tweets/) about attributes in Twitter archives downloaded by users , but at least we can see which attribute in WeRateDogs archive came from Twitter and which was added by WeRateDogs staff. Our main focus would be Tweet data retrieved via Twitter API, since Twitter has provided extensive online API documentation at developers' disposal:\n",
    "\n",
    "* https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet\n",
    "* https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/entities\n",
    "* https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/extended-entities\n",
    "\n",
    "We can understand the datatype and the contents of attributes by checking the details in the API documentation. Then it should take us little time to realize that the Twitter archive is a simplified version of full JSON data fetched via Twitter API, and by joining them together we can increase the tidyness of the dataset and reduce the number of duplicate, redundant columns. There are other identified tidyness problems, but these can be amended after the join is completed.\n",
    "\n",
    "After merging them into a single dataset and finished with tidyness problems, we can proceed with finding out quality problems like NULL values, inaccurate values and wrong datatypes, but there is a substantial quality issue we need to take care of: *Unwanted* [*types of Tweets*](https://help.twitter.com/en/using-twitter/types-of-tweets). Remember one of the key points mentioned in the project:\n",
    "\n",
    "*You only want original ratings (no retweets) that have images.*\n",
    "\n",
    "This implies the following types of Tweets should be dropped from the dataset:\n",
    "\n",
    "* [Retweets](https://help.twitter.com/en/using-twitter/retweet-faqs)\n",
    "* [Quote](https://www.lifewire.com/how-to-quote-a-tweet-on-twitter-4685775) [Tweets](https://members.tinshingle.com/content/what-quote-tweet-and-when-do-i-want-use-it-instead-retweet)\n",
    "* [Tweet](https://help.twitter.com/en/resources/twitter-guide/topics/how-to-join-the-conversation-on-twitter/how-to-reply-to-a-tweet-on-twitter) [Replies](https://help.twitter.com/en/using-twitter/mentions-and-replies)\n",
    "* Tweets with no native photo\n",
    "\n",
    "The problem of NULL values has been deferred until the final step of cleaning, because most of these columns are needed to identify unwanted Tweets. Considering the fact that Twitter is rolling out [Twitter API v2](https://developer.twitter.com/en/docs/twitter-api/early-access), we can expect most of the JSON designs will be overhauled in the near future, meaning the cleaned dataset we created is likely to become an one-time use product. Therefore, we choose to drop these columns which contain NULL values.\n",
    "\n",
    "And finally, with the Tweet dataset completely cleaned up, we can merge the [Tweet Image Predictions File](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv) into the Tweet dataset at ease, thus creating an unified master dataset. Notice this step is optional, as we may not need to use the image prediction dataset to produce insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
